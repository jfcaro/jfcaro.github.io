<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>Markmap</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.18.10/dist/style.css">
</head>
<body>
<svg id="mindmap"></svg>
<script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-view@0.18.10/dist/browser/index.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.18.10/dist/index.js"></script><script>(()=>{setTimeout(()=>{const{markmap:x,mm:K}=window,P=new x.Toolbar;P.attach(K);const F=P.render();F.setAttribute("style","position:absolute;bottom:20px;right:20px"),document.body.append(F)})})()</script><script>((b,L,T,D)=>{const H=b();window.mm=H.Markmap.create("svg#mindmap",(L||H.deriveOptions)(D),T)})(()=>window.markmap,null,{"content":"Modelos de Lenguaje Grandes (LLMs)","children":[{"content":"Introducci&#xf3;n (00:00:00)","children":[{"content":"Explicaci&#xf3;n general de los LLMs","children":[{"content":"Modelos de IA capaces de generar texto coherente","children":[],"payload":{"tag":"li","lines":"8,9"}},{"content":"Utilizados en diversas aplicaciones como chatbots, asistentes virtuales y an&#xe1;lisis de datos","children":[],"payload":{"tag":"li","lines":"9,10"}}],"payload":{"tag":"li","lines":"7,10"}},{"content":"Su capacidad y limitaciones","children":[{"content":"Capacidad de predecir palabras basadas en contexto previo","children":[],"payload":{"tag":"li","lines":"11,12"}},{"content":"Limitaciones en comprensi&#xf3;n real y razonamiento profundo","children":[],"payload":{"tag":"li","lines":"12,13"}}],"payload":{"tag":"li","lines":"10,13"}},{"content":"Importancia del contexto en su entrenamiento","children":[{"content":"La calidad de los datos influye en el desempe&#xf1;o del modelo","children":[],"payload":{"tag":"li","lines":"14,15"}},{"content":"Necesidad de preprocesamiento y filtrado para mejorar resultados","children":[],"payload":{"tag":"li","lines":"15,17"}}],"payload":{"tag":"li","lines":"13,17"}}],"payload":{"tag":"h2","lines":"6,7"}},{"content":"Preentrenamiento","children":[{"content":"Datos de preentrenamiento (00:01:00)","children":[{"content":"Recopilaci&#xf3;n de datos de internet","children":[{"content":"Fuentes como Wikipedia, libros digitalizados y foros","children":[],"payload":{"tag":"li","lines":"20,21"}}],"payload":{"tag":"li","lines":"19,21"}},{"content":"Filtrado de informaci&#xf3;n irrelevante o da&#xf1;ina","children":[{"content":"Eliminaci&#xf3;n de contenido ofensivo, repetitivo o poco &#xfa;til","children":[],"payload":{"tag":"li","lines":"22,23"}}],"payload":{"tag":"li","lines":"21,23"}},{"content":"Uso de conjuntos de datos como <strong>Common Crawl</strong> y <strong>FineWeb</strong>","children":[{"content":"Amplias bases de datos utilizadas en el entrenamiento de modelos de IA","children":[],"payload":{"tag":"li","lines":"24,26"}}],"payload":{"tag":"li","lines":"23,26"}}],"payload":{"tag":"h3","lines":"18,19"}},{"content":"Tokenizaci&#xf3;n (00:07:47)","children":[{"content":"Conversi&#xf3;n de texto en unidades m&#xe1;s peque&#xf1;as (tokens)","children":[{"content":"Fragmentaci&#xf3;n del texto en palabras o subpalabras","children":[],"payload":{"tag":"li","lines":"28,29"}}],"payload":{"tag":"li","lines":"27,29"}},{"content":"Algoritmos como <strong>Byte Pair Encoding (BPE)</strong>","children":[{"content":"M&#xe9;todo que reduce la cantidad de tokens y mejora la eficiencia","children":[],"payload":{"tag":"li","lines":"30,31"}}],"payload":{"tag":"li","lines":"29,31"}},{"content":"Importancia de la tokenizaci&#xf3;n en la eficiencia del modelo","children":[{"content":"Un buen esquema de tokenizaci&#xf3;n reduce el costo computacional","children":[],"payload":{"tag":"li","lines":"32,34"}}],"payload":{"tag":"li","lines":"31,34"}}],"payload":{"tag":"h3","lines":"26,27"}}],"payload":{"tag":"h2","lines":"17,18"}},{"content":"Redes Neuronales","children":[{"content":"Entrada y salida de la red neuronal (00:14:27)","children":[{"content":"Transformaci&#xf3;n de datos en representaciones num&#xe9;ricas","children":[{"content":"Conversi&#xf3;n de tokens en vectores multidimensionales","children":[],"payload":{"tag":"li","lines":"37,38"}}],"payload":{"tag":"li","lines":"36,38"}},{"content":"Predicci&#xf3;n del siguiente token","children":[{"content":"Uso de probabilidades para seleccionar la palabra m&#xe1;s adecuada","children":[],"payload":{"tag":"li","lines":"39,41"}}],"payload":{"tag":"li","lines":"38,41"}}],"payload":{"tag":"h3","lines":"35,36"}},{"content":"Internos de la red neuronal (00:20:11)","children":[{"content":"Uso del mecanismo de <strong>atenci&#xf3;n</strong>","children":[{"content":"Evaluaci&#xf3;n de la importancia de cada palabra en un contexto","children":[],"payload":{"tag":"li","lines":"43,44"}}],"payload":{"tag":"li","lines":"42,44"}},{"content":"Capacidad para capturar relaciones complejas entre palabras","children":[{"content":"Permite generar respuestas m&#xe1;s naturales y contextualmente adecuadas","children":[],"payload":{"tag":"li","lines":"45,47"}}],"payload":{"tag":"li","lines":"44,47"}}],"payload":{"tag":"h3","lines":"41,42"}}],"payload":{"tag":"h2","lines":"34,35"}},{"content":"Inferencia (00:26:01)","children":[{"content":"Proceso de generaci&#xf3;n de texto","children":[{"content":"Basado en las probabilidades de palabras calculadas por el modelo","children":[],"payload":{"tag":"li","lines":"49,50"}}],"payload":{"tag":"li","lines":"48,50"}},{"content":"Uso de distribuciones de probabilidad para elegir tokens","children":[{"content":"M&#xe9;todos como <strong>softmax</strong> para determinar la palabra m&#xe1;s adecuada","children":[],"payload":{"tag":"li","lines":"51,52"}}],"payload":{"tag":"li","lines":"50,52"}},{"content":"M&#xe9;todos de muestreo como <strong>Top-k Sampling</strong> y <strong>Temperatura</strong>","children":[{"content":"Controlan la creatividad y coherencia del texto generado","children":[],"payload":{"tag":"li","lines":"53,55"}}],"payload":{"tag":"li","lines":"52,55"}}],"payload":{"tag":"h2","lines":"47,48"}},{"content":"Casos de Estudio","children":[{"content":"Entrenamiento e inferencia de GPT-2 (00:31:09)","children":[{"content":"Modelo de 1.6B par&#xe1;metros","children":[{"content":"Primer modelo ampliamente disponible de OpenAI","children":[],"payload":{"tag":"li","lines":"58,59"}}],"payload":{"tag":"li","lines":"57,59"}},{"content":"Comparaci&#xf3;n con modelos modernos","children":[{"content":"Modelos recientes como GPT-4 tienen cientos de veces m&#xe1;s par&#xe1;metros","children":[],"payload":{"tag":"li","lines":"60,62"}}],"payload":{"tag":"li","lines":"59,62"}}],"payload":{"tag":"h3","lines":"56,57"}},{"content":"Inferencia en Llama 3.1 Base Model (00:42:52)","children":[{"content":"Diferencias con modelos comerciales","children":[{"content":"Llama 3.1 es un modelo de c&#xf3;digo abierto","children":[],"payload":{"tag":"li","lines":"64,65"}}],"payload":{"tag":"li","lines":"63,65"}},{"content":"Evaluaci&#xf3;n de su rendimiento","children":[{"content":"Comparaci&#xf3;n con modelos propietarios como ChatGPT","children":[],"payload":{"tag":"li","lines":"66,68"}}],"payload":{"tag":"li","lines":"65,68"}}],"payload":{"tag":"h3","lines":"62,63"}}],"payload":{"tag":"h2","lines":"55,56"}},{"content":"Postentrenamiento","children":[{"content":"Del preentrenamiento al postentrenamiento (00:59:23)","children":[{"content":"Ajuste fino para mejorar el rendimiento","children":[{"content":"Entrenamiento adicional en datos espec&#xed;ficos","children":[],"payload":{"tag":"li","lines":"71,72"}}],"payload":{"tag":"li","lines":"70,72"}},{"content":"Implementaci&#xf3;n de respuestas m&#xe1;s precisas","children":[{"content":"Adaptaci&#xf3;n del modelo para tareas especializadas","children":[],"payload":{"tag":"li","lines":"73,75"}}],"payload":{"tag":"li","lines":"72,75"}}],"payload":{"tag":"h3","lines":"69,70"}},{"content":"Datos de postentrenamiento (Conversaciones) (01:01:06)","children":[{"content":"Uso de conjuntos de datos conversacionales","children":[{"content":"Permiten mejorar la interacci&#xf3;n con humanos","children":[],"payload":{"tag":"li","lines":"77,78"}}],"payload":{"tag":"li","lines":"76,78"}},{"content":"Supervisi&#xf3;n humana para mejorar interacciones","children":[{"content":"Evaluaci&#xf3;n y ajuste manual de respuestas generadas","children":[],"payload":{"tag":"li","lines":"79,81"}}],"payload":{"tag":"li","lines":"78,81"}}],"payload":{"tag":"h3","lines":"75,76"}}],"payload":{"tag":"h2","lines":"68,69"}},{"content":"Limitaciones y Desaf&#xed;os","children":[{"content":"Alucinaciones, uso de herramientas y memoria de trabajo (01:20:32)","children":[{"content":"Tendencia a generar informaci&#xf3;n falsa","children":[{"content":"Puede inventar datos en ausencia de informaci&#xf3;n suficiente","children":[],"payload":{"tag":"li","lines":"84,85"}}],"payload":{"tag":"li","lines":"83,85"}},{"content":"M&#xe9;todos para reducir alucinaciones","children":[{"content":"Uso de t&#xe9;cnicas como verificaci&#xf3;n de fuentes y modelos h&#xed;bridos","children":[],"payload":{"tag":"li","lines":"86,88"}}],"payload":{"tag":"li","lines":"85,88"}}],"payload":{"tag":"h3","lines":"82,83"}},{"content":"Conocimiento de s&#xed; mismo (01:41:46)","children":[{"content":"Modelos no tienen autoconciencia","children":[{"content":"No comprenden su propia existencia ni intenciones","children":[],"payload":{"tag":"li","lines":"90,91"}}],"payload":{"tag":"li","lines":"89,91"}},{"content":"Simulan respuestas basadas en patrones","children":[{"content":"Basan sus respuestas en ejemplos previos sin razonamiento propio","children":[],"payload":{"tag":"li","lines":"92,94"}}],"payload":{"tag":"li","lines":"91,94"}}],"payload":{"tag":"h3","lines":"88,89"}},{"content":"Los modelos necesitan tokens para pensar (01:46:56)","children":[{"content":"Necesidad de contexto extenso","children":[{"content":"Mayor cantidad de tokens mejora la coherencia en respuestas largas","children":[],"payload":{"tag":"li","lines":"96,97"}}],"payload":{"tag":"li","lines":"95,97"}},{"content":"Limitaciones en comprensi&#xf3;n sin suficiente informaci&#xf3;n","children":[{"content":"Respuestas vagas o err&#xf3;neas si la entrada es insuficiente","children":[],"payload":{"tag":"li","lines":"98,100"}}],"payload":{"tag":"li","lines":"97,100"}}],"payload":{"tag":"h3","lines":"94,95"}},{"content":"Revisi&#xf3;n de la tokenizaci&#xf3;n: problemas con la ortograf&#xed;a (02:01:11)","children":[{"content":"Dificultades en procesar ortograf&#xed;a compleja","children":[{"content":"Puede fallar en deletrear palabras poco comunes","children":[],"payload":{"tag":"li","lines":"102,103"}}],"payload":{"tag":"li","lines":"101,103"}},{"content":"Ejemplo de errores en generaci&#xf3;n de palabras","children":[{"content":"Tendencia a generar palabras incorrectas o mal estructuradas","children":[],"payload":{"tag":"li","lines":"104,106"}}],"payload":{"tag":"li","lines":"103,106"}}],"payload":{"tag":"h3","lines":"100,101"}},{"content":"Inteligencia irregular (02:04:53)","children":[{"content":"Modelos sobresalen en algunas tareas pero fallan en otras","children":[{"content":"Buen desempe&#xf1;o en generaci&#xf3;n de texto, pero errores en matem&#xe1;ticas complejas","children":[],"payload":{"tag":"li","lines":"108,110"}}],"payload":{"tag":"li","lines":"107,110"}}],"payload":{"tag":"h3","lines":"106,107"}}],"payload":{"tag":"h2","lines":"81,82"}},{"content":"Aprendizaje y Refinamiento","children":[{"content":"Ajuste fino supervisado hasta aprendizaje por refuerzo (02:07:28)","children":[{"content":"Diferencias entre los m&#xe9;todos de entrenamiento","children":[{"content":"Supervisi&#xf3;n manual vs. ajustes din&#xe1;micos con retroalimentaci&#xf3;n","children":[],"payload":{"tag":"li","lines":"113,114"}}],"payload":{"tag":"li","lines":"112,114"}},{"content":"Comparaci&#xf3;n de rendimiento","children":[{"content":"Modelos refinados mediante refuerzo suelen ser m&#xe1;s precisos","children":[],"payload":{"tag":"li","lines":"115,117"}}],"payload":{"tag":"li","lines":"114,117"}}],"payload":{"tag":"h3","lines":"111,112"}},{"content":"Aprendizaje por refuerzo (02:14:42)","children":[{"content":"M&#xe9;todos para mejorar generaci&#xf3;n de respuestas","children":[{"content":"T&#xe9;cnicas avanzadas como el refuerzo con retroalimentaci&#xf3;n humana","children":[],"payload":{"tag":"li","lines":"119,121"}}],"payload":{"tag":"li","lines":"118,121"}}],"payload":{"tag":"h3","lines":"117,118"}},{"content":"DeepSeek-R1 (02:27:47)","children":[{"content":"Caso de estudio en entrenamiento y despliegue","children":[{"content":"Evaluaci&#xf3;n de su eficiencia en diferentes tareas","children":[],"payload":{"tag":"li","lines":"123,125"}}],"payload":{"tag":"li","lines":"122,125"}}],"payload":{"tag":"h3","lines":"121,122"}},{"content":"AlphaGo (02:42:07)","children":[{"content":"Uso de IA en juegos estrat&#xe9;gicos","children":[{"content":"&#xc9;xito de AlphaGo en derrotar campeones humanos","children":[],"payload":{"tag":"li","lines":"127,128"}}],"payload":{"tag":"li","lines":"126,128"}},{"content":"Comparaciones con modelos de lenguaje","children":[{"content":"Similitudes en el aprendizaje por refuerzo","children":[],"payload":{"tag":"li","lines":"129,131"}}],"payload":{"tag":"li","lines":"128,131"}}],"payload":{"tag":"h3","lines":"125,126"}},{"content":"Aprendizaje por refuerzo con retroalimentaci&#xf3;n humana (RLHF) (02:48:26)","children":[{"content":"Implementaci&#xf3;n en modelos modernos","children":[{"content":"T&#xe9;cnica clave en modelos como ChatGPT","children":[],"payload":{"tag":"li","lines":"133,134"}}],"payload":{"tag":"li","lines":"132,134"}},{"content":"Mejora de la alineaci&#xf3;n con respuestas humanas","children":[{"content":"Reducci&#xf3;n de sesgos y generaci&#xf3;n de respuestas m&#xe1;s &#xfa;tiles","children":[],"payload":{"tag":"li","lines":"135,137"}}],"payload":{"tag":"li","lines":"134,137"}}],"payload":{"tag":"h3","lines":"131,132"}}],"payload":{"tag":"h2","lines":"110,111"}},{"content":"Estado Actual y Futuro","children":[{"content":"Avance de la tecnolog&#xed;a (03:09:39)","children":[{"content":"Predicciones sobre la evoluci&#xf3;n de LLMs","children":[{"content":"Mayor capacidad de procesamiento y reducci&#xf3;n de costos","children":[],"payload":{"tag":"li","lines":"140,141"}}],"payload":{"tag":"li","lines":"139,141"}},{"content":"&#xc1;reas de mejora y desarrollo","children":[{"content":"Optimizaci&#xf3;n en eficiencia energ&#xe9;tica y reducci&#xf3;n de alucinaciones","children":[],"payload":{"tag":"li","lines":"142,144"}}],"payload":{"tag":"li","lines":"141,144"}}],"payload":{"tag":"h3","lines":"138,139"}},{"content":"Seguimiento de LLMs (03:15:15)","children":[{"content":"M&#xe9;todos para evaluar el rendimiento de los modelos","children":[{"content":"Uso de m&#xe9;tricas como <strong>perplejidad</strong> y pruebas en entornos reales","children":[],"payload":{"tag":"li","lines":"146,148"}}],"payload":{"tag":"li","lines":"145,148"}}],"payload":{"tag":"h3","lines":"144,145"}},{"content":"D&#xf3;nde encontrar LLMs (03:18:34)","children":[{"content":"Fuentes abiertas y comerciales","children":[{"content":"Modelos disponibles en plataformas como Hugging Face y OpenAI","children":[],"payload":{"tag":"li","lines":"150,152"}}],"payload":{"tag":"li","lines":"149,152"}}],"payload":{"tag":"h3","lines":"148,149"}},{"content":"Resumen general (03:21:46)","children":[{"content":"Revisi&#xf3;n de las ideas principales","children":[{"content":"Impacto en sociedad, industria y &#xe9;tica","children":[],"payload":{"tag":"li","lines":"154,155"}}],"payload":{"tag":"li","lines":"153,155"}},{"content":"Impacto de los LLMs en la sociedad y la tecnolog&#xed;a","children":[{"content":"Transformaci&#xf3;n de m&#xfa;ltiples sectores gracias a la IA","children":[],"payload":{"tag":"li","lines":"156,158"}}],"payload":{"tag":"li","lines":"155,158"}}],"payload":{"tag":"h3","lines":"152,153"}}],"payload":{"tag":"h2","lines":"137,138"}}]},{"colorFreezeLevel":2})</script>
</body>
</html>
